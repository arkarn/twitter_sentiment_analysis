{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hdbcli import dbapi\n",
    "import hana_credentials\n",
    "from sqlalchemy import create_engine\n",
    "from preprocess_tweets import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "conn = dbapi.connect(\n",
    "    address=hana_credentials.ADDRESS, \n",
    "    port=hana_credentials.PORT, \n",
    "    user=hana_credentials.USER, \n",
    "    password=hana_credentials.PASSWORD\n",
    ")\n",
    "\n",
    "print(conn.isconnected() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine = create_engine(hana_credentials.ENGINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run live_stream_tweets.py from cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table created...\n"
     ]
    }
   ],
   "source": [
    "cursor = conn.cursor() \n",
    "cursor.execute(\"\"\"Create column table twitter  (ID int NOT NULL primary key GENERATED BY DEFAULT AS IDENTITY, USERNAME Nvarchar(25),CREATED_AT datetime,TEXT nvarchar(200),HASHTAGS nvarchar(150)) \"\"\")\n",
    "print ('table created...' )\n",
    "\n",
    "# #insert some data into table \n",
    "# stmnt = 'insert into I341495.twitter values (?, ?)' \n",
    "# cursor.execute(stmnt, (1,'A')) \n",
    "# cursor.execute(stmnt, (2,'B')) \n",
    "# print ('2 records inserted into table') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@marcobonzanini', ':', 'just', 'an', 'example', '!', ':D', 'http://example.com', '#NLP']\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "tweet = 'RT @marcobonzanini: just an example! :D http://example.com #NLP'\n",
    "print(preprocess(tweet))\n",
    "# ['RT', '@marcobonzanini', ':', 'just', 'an', 'example', '!', ':D', 'http://example.com', '#NLP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "username: 4orgexcellence\n",
      "text: Nice quick lesson What Data Scientist 60 seconds\n",
      "created_at: 2018-04-10 11:11:43+00:00\n",
      "hashtags: DataScience,ML,BigData\n",
      "2\n",
      "username: katsuhitosudoh\n",
      "text: A convnet chainer title page NICT magazine month :) NICT one Japan's three big computer science\n",
      "created_at: 2018-04-10 11:11:47+00:00\n",
      "hashtags: \n",
      "3\n",
      "username: AkumuFiona\n",
      "text: What freelancer improve cash flow gt\n",
      "created_at: 2018-04-10 11:12:03+00:00\n",
      "hashtags: javascript,vuejs,code,php,angular\n",
      "4\n",
      "username: Mada_GD\n",
      "text: Unsupervised challenge\n",
      "created_at: 2018-04-10 11:12:08+00:00\n",
      "hashtags: MachineLearning,DataScience,BigData,VinodsBlog,AILabPage\n",
      "5\n",
      "username: mclynd\n",
      "text: A little recap Kickstarter Indiegogo\n",
      "created_at: 2018-04-10 11:12:10+00:00\n",
      "hashtags: opensource,opensourceai,ai,voice\n",
      "6\n",
      "username: MangoTheCat\n",
      "text: Updated training course list website Ta\n",
      "created_at: 2018-04-10 11:12:49+00:00\n",
      "hashtags: rstats,python,machinelearning,shiny\n",
      "7\n",
      "username: GeroldPenz\n",
      "text: Das nächste Meetup Innsbruck findet 17.4 statt\n",
      "created_at: 2018-04-10 11:12:54+00:00\n",
      "hashtags: Python,ibktwit,intirol\n",
      "8\n",
      "username: fan_django\n",
      "text: Practical OpenCV 3 Image Processing Python ☞\n",
      "created_at: 2018-04-10 11:13:27+00:00\n",
      "hashtags: django,python\n",
      "9\n",
      "username: tsun_kirapon\n",
      "text: あと142人で執筆開始 ！ ブロックチェーンの理論と実装を理解する入門書 「 ゼロから創る暗号通貨 」 執筆プロジェクト ！\n",
      "created_at: 2018-04-10 11:13:39+00:00\n",
      "hashtags: Python,peaks_cc,技術書クラウドファンディング\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from dateutil.parser import parse\n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via', '...', 'RT','…']\n",
    "\n",
    "def cleanText(text):\n",
    "    text_list=[term for term in preprocess(tweet['text']) if term not in stop and not term.startswith(('#', '@', 'http'))]\n",
    "    return \" \".join(text_list)\n",
    "\n",
    "\n",
    "i=0\n",
    "with open('live_tweets_hana.json', 'r') as f:\n",
    "    lines = filter(None, (line.rstrip() for line in f))\n",
    "    for line in lines: # read only the first tweet/line\n",
    "        tweet = json.loads(line) # load it as Python dict\n",
    "        #print(json.dumps(tweet, indent=4)) # pretty-print\n",
    "        i+=1\n",
    "        print(i)\n",
    "        print('username:',tweet['user']['screen_name'])\n",
    "        print('text:',cleanText(tweet['text']))\n",
    "        print('created_at:', parse(tweet['created_at']))\n",
    "        #hashtags\n",
    "        hashtags=''\n",
    "        for hashtag in tweet['entities']['hashtags']:\n",
    "            hashtags+=hashtag['text']+','\n",
    "            \n",
    "        hashtags=hashtags[:-1]#removing last comma\n",
    "        print('hashtags:', hashtags)\n",
    "        #print(json.dumps(tweet['user'], indent=4))\n",
    "        #print(tweet['user']['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('！', 2), ('A', 2), ('What', 2), ('NICT', 2), ('little', 1)]\n"
     ]
    }
   ],
   "source": [
    "import operator \n",
    "import json\n",
    "from collections import Counter\n",
    " \n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via', '...', 'RT','…']\n",
    "\n",
    "fname = 'live_tweets_hana.json'\n",
    "with open(fname, 'r') as f:\n",
    "    lines = filter(None, (line.rstrip() for line in f))\n",
    "    count_all = Counter()\n",
    "    for line in lines:\n",
    "        tweet = json.loads(line)\n",
    "        # Create a list with all the terms\n",
    "        #terms_all = [term for term in preprocess(tweet['text'])]\n",
    "        terms_stop = [term for term in preprocess(tweet['text']) if term not in stop and not term.startswith(('#', '@'))]\n",
    "        # Update the counter\n",
    "        count_all.update(terms_stop)\n",
    "    # Print the first 5 most frequent words\n",
    "    print(count_all.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 4, 10, 11, 11, 47, tzinfo=tzutc())"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dateutil.parser import parse\n",
    "parse('Tue Apr 10 11:11:47 +0000 2018')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
